{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Over the Decades\n",
    "\n",
    "Music has been a significant part of society for thousands of years. As a result, humanity has seen a wide varitey of styles, instruments, genres, etc. In the last century, the world of music has seen many changes. Technology allowed music to be recorded and played on discs. Now music can easily be streamed from personal devices through services like Spotify. The technologies for making music changed too. Computers can now be used to create and mix sounds, rhythms, and whole songs.\n",
    "\n",
    "So how has music changed? Can we see changes by looking at data? What kinds of relationships does music have with the time period in which it is created? These are some questions we hope to answer by going through the data science pipeline.\n",
    "\n",
    "In this tutorial, we will use the Spotify API to look at popular music over the decades. Specifically, we will look at the All Out 50's, 60's, 70's, 80's, 90's, 00's, and 10's playlists created by Spotify. This should provide us with a sufficient overview of popular music in the last century.\n",
    "\n",
    "\n",
    "### Part 1: Data Collection\n",
    "\n",
    "First things first, we need to collect our data. We will be using the Spotipy python library to interact with Spotify. The following steps allow us to use the Spotify API. This can easily be found on the Spotipy website which I have linked at the end of this tutorial. The environment variables listed below are for my personal Spotify account. You will have your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spotipy\n",
      "  Downloading spotipy-2.16.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /opt/conda/lib/python3.8/site-packages (from spotipy) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.20.0 in /opt/conda/lib/python3.8/site-packages (from spotipy) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->spotipy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->spotipy) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->spotipy) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20.0->spotipy) (1.25.10)\n",
      "Installing collected packages: spotipy\n",
      "Successfully installed spotipy-2.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spotipy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import pandas as pd\n",
    "\n",
    "auth_manager = SpotifyClientCredentials()\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have access to Spotiy, we need to get our playlists and songs along with all of their data so we can perform some analysis. The two blocks of code below are functions found from another tutorial on How to Create Large Music Datasets Using Spotipy by Max Hilsdorf, link here: https://towardsdatascience.com/how-to-create-large-music-datasets-using-spotipy-40e7242cc6a6\n",
    "\n",
    "I have made some minor modifications to these functions, but they operate the same way as in the linked tutorial. We will use the analyze_playlist function to loop over all tracks in a playlist and store specific data related to each song in a dataframe. The second function, analyze_playlist_dict, will be used to concatenate the dataframes from each individual playlist into one large dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_playlist(playlist_id):\n",
    "    \n",
    "    # Create empty dataframe\n",
    "    playlist_features_list = [\"artist\", \"album\", \"track_name\", \"track_id\", \"popularity\", \"explicit\",\n",
    "                              \"danceability\", \"energy\", \"speechiness\", \"instrumentalness\",\n",
    "                              \"acousticness\", \"liveness\", \"loudness\", \"valence\", \n",
    "                              \"mode\", \"key\", \"tempo\", \"duration_ms\", \"time_signature\"]\n",
    "    \n",
    "    playlist_df = pd.DataFrame(columns = playlist_features_list)\n",
    "    \n",
    "    # Loop through every track in the playlist, extract features and append the features to the playlist df\n",
    "    playlist = sp.playlist_tracks(playlist_id)[\"items\"]\n",
    "    for track in playlist:\n",
    "        # Create empty dict\n",
    "        playlist_features = {}\n",
    "        # Get metadata\n",
    "        playlist_features[\"artist\"] = track[\"track\"][\"album\"][\"artists\"][0][\"name\"]\n",
    "        playlist_features[\"album\"] = track[\"track\"][\"album\"][\"name\"]\n",
    "        playlist_features[\"track_name\"] = track[\"track\"][\"name\"]\n",
    "        playlist_features[\"track_id\"] = track[\"track\"][\"id\"]\n",
    "        playlist_features[\"popularity\"] = track[\"track\"][\"popularity\"]\n",
    "        playlist_features[\"explicit\"] = track[\"track\"][\"explicit\"]\n",
    "        \n",
    "        # Get audio features\n",
    "        audio_features = sp.audio_features(playlist_features[\"track_id\"])[0]\n",
    "        for feature in playlist_features_list[6:]:\n",
    "            playlist_features[feature] = audio_features[feature]\n",
    "        \n",
    "        # Concat the dfs\n",
    "        track_df = pd.DataFrame(playlist_features, index = [0])\n",
    "        playlist_df = pd.concat([playlist_df, track_df], ignore_index = True)\n",
    "        \n",
    "    return playlist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_playlist_dict(playlist_dict):\n",
    "    i = 0\n",
    "    # Loop through every playlist in the dict and analyze it\n",
    "    for name, playlist_id in playlist_dict.items():\n",
    "        playlist_df = analyze_playlist(playlist_id)\n",
    "        # Add a playlist column so that we can see which playlist a track belongs too\n",
    "        playlist_df[\"playlist\"] = name\n",
    "        #Create or concat df\n",
    "        if i == 0:\n",
    "            playlist_dict_df = playlist_df\n",
    "        else:\n",
    "            playlist_dict_df = pd.concat([playlist_dict_df, playlist_df], ignore_index = True)\n",
    "        i += 1\n",
    "        \n",
    "    return playlist_dict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to access playlists, we need their playlist URI's. These can easily be found in the Spotify desktop app. The seven URI's for the mentioned playlists are stored in a dictionary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "decades_URIs = {\n",
    "    '50s' : 'spotify:playlist:37i9dQZF1DWSV3Tk4GO2fq',\n",
    "    '60s' : 'spotify:playlist:37i9dQZF1DXaKIA8E7WcJj',\n",
    "    '70s' : 'spotify:playlist:37i9dQZF1DWTJ7xPn4vNaz',\n",
    "    '80s' : 'spotify:playlist:37i9dQZF1DX4UtSsGT1Sbe',\n",
    "    '90s' : 'spotify:playlist:37i9dQZF1DXbTxeAdrVG2l',\n",
    "    '00s' : 'spotify:playlist:37i9dQZF1DX4o1oenSJRJd',\n",
    "    '10s' : 'spotify:playlist:37i9dQZF1DX5Ejj0EkURtP'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we have to do is pass this dict into the second function above to get our beast of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analyze_playlist_dict(decades_URIs)\n",
    "\n",
    "#pd.set_option('display.max_rows', None)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Cleaning\n",
    "\n",
    "We want this data looking nice before we start graphing it, so we need to clean it. Thankfully, Spotify is thorough so the data we have is pretty complete and does not require much cleaning at all. One thing we will do is change the duration of the songs from milliseconds to minutes. We also have to adjust the typing of some of the categories so they can be graphed properly. Other than that, we have no NaN values to take care of and no strange data so we are done with that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['duration_ms'] = df['duration_ms'].astype('float').div(60000) # change duration to minutes\n",
    "df.rename(columns={'duration_ms' : 'duration_min'}, inplace=True)\n",
    "df['popularity'] = df['popularity'].astype(float)\n",
    "df['instrumentalness'] = df['instrumentalness'].astype(float)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let's prepare a dataframe with exclusively numerical data. This will be useful later on when do some machine learning. We'll just copy the dataframe and drop the non-numercal columns like name. We'll also drop mode and time signature as those are not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = df.copy()\n",
    "numerical.drop(columns=['artist', 'album', 'track_name', 'track_id', 'explicit', 'mode', 'time_signature'], inplace=True)\n",
    "display(numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Exploration and Vizualization\n",
    "\n",
    "Let's take our data and make some plots. We can plot a number of variables against each other here, but since we are looking at music over the decades let us use the decade as one of the variables. Information about each variable can be found in the Spotify API documentation, links are below.\n",
    "\n",
    "First we'll look at danceability. This is a measure of how good a track is for dancing and ranges 0.0 to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style='whitegrid', palette='bright') # style of the plot, bright colors look nice\n",
    "\n",
    "plt.title('violin plot of danceability over the decades') # want to make sure we title our plots\n",
    "ax = sns.violinplot(x=df['playlist'], y=df['danceability']) # violin plot looking at danceability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this plot, we can see that music in the 80s and 10s is more danceable than other decades.\n",
    "\n",
    "Next we'll look at acousticness. Higher values represent higher confidence in whether a track is acoustic. This ranges 0.0 to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('violin plot of acousticness over the decades')\n",
    "ax = sns.violinplot(x=df['playlist'], y=df['acousticness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of this plot is interesting. We see that older songs (50's and 60's) are much more likely to be acoustic than newer songs (90's and up). \n",
    "\n",
    "Let's look at loudness of songs. The loudness values are averaged across the whole track for comparison's sake. Values typically range -60 db to 0 db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('violin plot of loudness over the decades')\n",
    "ax = sns.violinplot(x=df['playlist'], y=df['loudness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a general trend of increase in the loudness over the decades, although the 80's see a slight dip compared to surrounding decades. The 00's seem to have the loudest music. \n",
    "\n",
    "Now let's look at the valence. This is a measure of the musical positiveness of a track. Higher valence equates to happier songs. This ranges 0.0 to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('violin plot of valence over the decades')\n",
    "ax = sns.violinplot(x=df['playlist'], y=df['valence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution shows high valence early on, peaking in the 80's then dropping hard into the 90's and forward.\n",
    "\n",
    "Next up is song duration. This is measured in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('violin plot of duration (minutes) of songs over the decades')\n",
    "ax = sns.violinplot(x=df['playlist'], y=df['duration_min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Song duration appears to have been low (under 3 mins) early on then rose above 4 minutes for the 70's through 90's, then lowered back down a bit in the following years.\n",
    "\n",
    "We can look at popularity of songs, however the popularity is determined by how much a song has been listened to recently and will therefore likely favor newer songs. Popularity is an integer ranging 0 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('violin plot of popularity over the decades')\n",
    "ax = sns.violinplot(x=df['playlist'], y=df['popularity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10's have a clear lead over the other decades as far as popularity goes. This difference is perhaps less than what could be expected. The older decades seem to be quite popular still as well, and the relationship between time and popularity is quite weak. This suggests that people listen to the music they like, not necessarily what is new.\n",
    "\n",
    "We will now look at explicit music over the decades. This is a boolean category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('bar plot of explicit songs over the decades')\n",
    "ax = sns.barplot(x=df['playlist'], y=df['explicit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart is not too exciting, but it does show us something interesting. There are no explicit songs until the 00's, then the number jumps. This implies that explicit music was not popular until the 00's, then became a part of the mainstream. \n",
    "\n",
    "Next we will relate different variables to get a general look at music. First we will plot energy of music versus loudness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=df['loudness'], y=df['energy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a pretty solid linear relationship here, implying that louder music is more energetic. This certainly seems like it makes sense as a soft lulaby is not as energetic as deafening rave music.\n",
    "\n",
    "Another relationship we'll look at is loudness versus danceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=df['loudness'], y=df['danceability'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship here is a bit weaker than the previous.\n",
    "\n",
    "Just for fun let's look at the top artists and the top keys of our tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['artist'].value_counts().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_class = {\n",
    "    0 : 'C',\n",
    "    1 : 'C#/Db',\n",
    "    2 : 'D',\n",
    "    3 : 'D#/Eb',\n",
    "    4 : 'E',\n",
    "    5 : 'F',\n",
    "    6 : 'F#/Gb',\n",
    "    7 : 'G',\n",
    "    8 : 'G#/Ab',\n",
    "    9 : 'A',\n",
    "    10 : 'A#/Bb',\n",
    "    11 : 'B'\n",
    "}\n",
    "\n",
    "vc = pd.DataFrame()\n",
    "vc = df['key'].value_counts()\n",
    "vc.index = vc.index.map(pitch_class.get)\n",
    "vc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Analysis\n",
    "\n",
    "Now that we've done some exciting exploration and made some pretty graphs, let's see if we can move on to something more useful. Since we're looking at decades of music and each decade is has a different 'sound', a natural question arises: can we predict which decade a track is from based on its musical characteristics. We'll apply some machine learning techniques to answer this.\n",
    "\n",
    "There are two ways we can go about this:\n",
    "1) We can take some songs off each decades playlist to use as testing data and keep the rest for training data.\n",
    "\n",
    "2) We can use all of the data we have here for training data and find a new playlist for testing data.\n",
    "\n",
    "Since we have a large amount of data already clean and ready to use, we will go with the first option. An extra 20 songs likely will not make too much of a difference here. We will however use two methods of splitting the data: we will manually split the data such that we get a random 80:20 from each decade, and we will also use 10-fold cross validation which will not have the even decades split.\n",
    "\n",
    "For good measure, we'll use two types of machine learning: decision trees and support vector machines.\n",
    "\n",
    "Here we will split the data into training and testing sets. We'll use a split of 80:20 for this. Since our data has 700 tracks, we'll have 140 for testing, 20 from each decade. This is where the numerical dataframe we made earlier will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create the training and testing frames\n",
    "X_train = pd.DataFrame() \n",
    "X_test = pd.DataFrame()\n",
    "\n",
    "group = numerical.groupby(['playlist']) # group the data by decade\n",
    "\n",
    "i = 0\n",
    "for playlist, g in group: # for each decade\n",
    "    X = g.copy()\n",
    "    y = X['playlist']\n",
    "    \n",
    "    X_t, X_te, y_t, y_tr = train_test_split(X, y, random_state=1, test_size=0.20) # gets a random 80:20 train-test split\n",
    "    \n",
    "    # compile the train and test data from each decade into two larger dataframes\n",
    "    if i == 0:\n",
    "        X_train = X_t\n",
    "        X_test = X_te\n",
    "    else:\n",
    "        X_train = pd.concat([X_train, X_t], ignore_index=True)\n",
    "        X_test = pd.concat([X_test, X_te], ignore_index=True)\n",
    "    i += 1\n",
    "    \n",
    "# here we set the y values for the train and test sets, then drop the 'playlist' label from the X frames\n",
    "y_train = X_train['playlist']\n",
    "X_train.drop(columns='playlist', inplace=True)\n",
    "y_test = X_test['playlist']\n",
    "X_test.drop(columns='playlist', inplace=True)\n",
    "\n",
    "# this set is to be used for 10-fold cross validation\n",
    "X = numerical.copy()\n",
    "y = X['playlist']\n",
    "X.drop(columns='playlist', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the decision tree, we will create the model, train it on the training data, then test it on the test data. We will also perform a 10-fold cross validation on this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy import stats\n",
    "\n",
    "    \n",
    "dtc = tree.DecisionTreeClassifier() # creates the decision tree model\n",
    "dtc_fit = dtc.fit(X_train, y_train) # fit the model\n",
    "\n",
    "y_predict = dtc_fit.predict(X_test) # apply the model to the test set\n",
    "\n",
    "acc = accuracy_score(y_test, y_predict) # check the accuracy of the model test results\n",
    "\n",
    "scoresT = cross_val_score(dtc, X, y, cv=10) # 10-fold cross validation\n",
    "\n",
    "print(\"Accuracy: %0.2f\" % acc) #\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresT.mean(), scoresT.std() * 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy score of ~36% is not too great. However, randomly guessing has ~14% accuracy score and 36 is more than double that, so obviously the model does something.\n",
    "\n",
    "The accuracy for 10-fold cross validation is higher than the train test accuracy score. This makes sense because each fold removes a section of the data to use for testing, and it is likely that that section will largely be from one decade. Basically, because the train testing data in the cross validation is not evenly distributed over decades, we can expect a higher accuracy. Also notice that the error on this accuracy is not insignificant.\n",
    "\n",
    "Now we'll use the support vector machines model. The accuracy here does not change between iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svc = svm.SVC()\n",
    "svc_fit = svc.fit(X_train, y_train)\n",
    "\n",
    "y_predict = svc_fit.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_predict) # accuracy of model test results\n",
    "\n",
    "scoresS = cross_val_score(svc, X, y, cv=10) # 10-fold cross validation\n",
    "\n",
    "print(\"Accuracy: %0.2f\" % acc)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresS.mean(), scoresS.std() * 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here a lower accuracy of ~27%. This accuracy is still better than guessing, but not good. The cross validation accuracy is much closer now to the train-test split accuracy and is actually slightly lower. This is likely due to how the model functions.\n",
    "\n",
    "\n",
    "Unfortunately, neither of our models have particularly high accuracies. This basically means music is not particularly predictable with respects to time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = stats.ttest_rel(scoresT, scoresS)\n",
    "print(\"p-value: %0.3f\" % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have done a quick paired t-test on the results of the two 10-fold cross validations. The p-value is less than 0.05 which means there is likely no relationship between the outcomes of these two models. This checks out because we are applying two different machine learning models to the data and neither one is particularly good at predicting the decades of music, and this more or less validates our assumption from above about music being poorly predictable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Interpretation\n",
    "\n",
    "From the analysis we have done, it seems like different characteristics of music have different relationships with music over time. For example, we can see from the violin plots that acousticness has a noticble decrease over time and explicitness strictly increases with the decades. But then looking at something like popularity, we can see that there is not necesarily a relationship with time, since people tend to listen to the time period of music which they prefer.\n",
    "\n",
    "Because of the significant variety in how musical characteristics relate to time period, we had trouble finding a model that could predict the decade of a song based on its audio features. The models we created had low accuracy, and while better than guessing, were certainly not good predictors.\n",
    "\n",
    "A way in which this could be improved (and I leave this as a challenge to the reader) would be to create models for each feature of the tracks i.e. a model for danceability, a model for valence, etc. I estimate some of these models would give significantly better predictions, such as the model for acousticness, whereas others would likely not have any serious improvement over the ones we created already.\n",
    "\n",
    "Another potential change for this analysis would be to get music from similar genres. The songs we collected are all popular songs from their specific time period, but cover a wide variety of genres, which results in many different sounds even within a decade. Selecting one or a few genres to analyze over the decades would then look at how that specific genre changed over the years and may yield some interesting results.\n",
    "\n",
    "While we were not able to predict the decade of music based on its characteristics, we certainly have left some doors open for further exploration into this area. \n",
    "\n",
    "Music always has been and always will be a major part of people's lives. Technology is advancing more and more everyday, allowing for new and exciting ways to make music as well as listen to music. Companies like Spotify providing the data we used have created some interesting opportunities for some serious analysis of the music industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Resources\n",
    "\n",
    "Guide to using Spotify API: https://towardsdatascience.com/how-to-create-large-music-datasets-using-spotipy-40e7242cc6a6\n",
    "\n",
    "Spotipy documentation: https://spotipy.readthedocs.io/en/2.16.1/#api-reference\n",
    "\n",
    "Track object: https://developer.spotify.com/documentation/web-api/reference/tracks/get-track/\n",
    "\n",
    "Audio features descriptions: https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
